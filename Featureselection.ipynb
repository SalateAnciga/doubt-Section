{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dc7407-241b-4a77-a157-ddbeee8eca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4b1deb-3b46-4062-830e-bc6ecbd2233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARDIZATION FUNCTION\n",
    "def split_scalar(indep_x,dep_y):\n",
    "      X_train, X_test, y_train, y_test = train_test_split(indep_x, dep_y, test_size = 0.25, random_state = 0)#split training set and test set\n",
    "      sc = StandardScaler() # call standardization function\n",
    "      X_train = sc.fit_transform(X_train)#find mean and standard divation for x_train\n",
    "      X_test = sc.transform(X_test)    \n",
    "      return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aaddac1-5f18-432c-b59d-0b3f46ed028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTKBEST FUNCTION\n",
    "def selectkbest(indep_x,dep_y,n):#Create a function selectkBest with 3 parameter\n",
    "      test = SelectKBest(score_func=chi2, k=n)#select best feature using chi squared\n",
    "      fit1= test.fit(indep_x,dep_y)#create a model based on best feature\n",
    "      selectk_features = fit1.transform(indep_x)#Create a new indep_x dataset based on k values\n",
    "      return selectk_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01e49f3-3e80-4105-9bfb-a5b3781c3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIND EVALUATION METRICS FOR REGRESSION\n",
    "def r2_prediction(regressor,X_test,y_test):#Create a function r2_prediction with 3 arguments\n",
    "     y_pred = regressor.predict(X_test)#findout the predicted output based on X_test(input)\n",
    "     r2=r2_score(y_test,y_pred)#Findout r2_score value \n",
    "     return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50e542a-5f73-47a7-b160-05cb19acdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGRESSION ALGORITHM\n",
    "def Linear(X_train,y_train,X_test):       \n",
    "        regressor = LinearRegression()#Call linearregression fun\n",
    "        regressor.fit(X_train, y_train)#Create a model\n",
    "        r2=r2_prediction(regressor,X_test,y_test)#call r2_prediction & findout r2 value \n",
    "        return  r2   \n",
    "\n",
    "def svm_linear(X_train,y_train,X_test):\n",
    "        regressor = SVR(kernel = 'linear')\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2  \n",
    "\n",
    "def svm_NL(X_train,y_train,X_test):   \n",
    "        regressor = SVR(kernel = 'rbf')\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2    \n",
    "    \n",
    "def Decision(X_train,y_train,X_test):\n",
    "        regressor = DecisionTreeRegressor(random_state = 0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2 \n",
    "    \n",
    "def random(X_train,y_train,X_test):       \n",
    "        regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "        regressor.fit(X_train, y_train)\n",
    "        r2=r2_prediction(regressor,X_test,y_test)\n",
    "        return  r2 \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88442903-d8e2-472d-bffa-3741a7d943b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE A TABLE\n",
    "def selectk_regression(acclin,accsvml,accsvmnl,accdes,accrf): #Create a function  with 5 arguments\n",
    "    #Create a table,row named as Chisquare,column named as linear,SVMl,SVMnl,Decision,Random  \n",
    "    dataframe=pd.DataFrame(index=['ChiSquare'],columns=['Linear','SVMl','SVMnl','Decision','Random'])                                                                                     \n",
    "    for number,idex in enumerate(dataframe.index):#In the for loop  enumerate(dataframe.index) gives ChiSquare,number start from 0to 4\n",
    "        dataframe['Linear'][idex]=acclin[number] # #dataframe['Linear'][ChiSquare]=r2lin[0]    \n",
    "        dataframe['SVMl'][idex]=accsvml[number]#dataframe[SVMl'][ChiSquare]=r2svml[1]\n",
    "        dataframe['SVMnl'][idex]=accsvmnl[number]#dataframe['SVMnl'][ChiSquare]=r2svmnl[2]\n",
    "        dataframe['Decision'][idex]=accdes[number]#dataframe['Decision'][ChiSquare]=r2des[3]\n",
    "        dataframe['Random'][idex]=accrf[number]#dataframe['r2rf'][ChiSquare]=r2rf[4]\n",
    "    return dataframerfedataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80367842-dec8-4ff6-8662-7f329f1fca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"teen_phone_addiction_dataset.csv\", index_col=None)\n",
    "df2.drop(['Name', 'Location'], axis=1, inplace=True)\n",
    "#df2= pd.get_dummies(df2,columns=['Gender','School_Grade','Phone_Usage_Purpose'],dtype=int,drop_first=True)\n",
    "df2=pd.get_dummies(df2,dtype=int,drop_first=True)\n",
    "# Step : Define independent and target variables\n",
    "indep_x = df2.drop('Addiction_Level',axis=1)\n",
    "dep_y = df2['Addiction_Level']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5000cb1-52f2-4e08-b3ab-4eaf1062b769",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([10. , 10. ,  9.2, ...,  6.2, 10. ,  6.3]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kbest\u001b[38;5;241m=\u001b[39mselectkbest(indep_x,dep_y,\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m# call the fun & put k=5    \u001b[39;00m\n\u001b[0;32m      3\u001b[0m acclin\u001b[38;5;241m=\u001b[39m[]\u001b[38;5;66;03m#Create a empty list\u001b[39;00m\n\u001b[0;32m      4\u001b[0m accsvml\u001b[38;5;241m=\u001b[39m[] \n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mselectkbest\u001b[1;34m(indep_x, dep_y, n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselectkbest\u001b[39m(indep_x,dep_y,n):\u001b[38;5;66;03m#Create a function selectkBest with 3 parameter\u001b[39;00m\n\u001b[0;32m      3\u001b[0m       test \u001b[38;5;241m=\u001b[39m SelectKBest(score_func\u001b[38;5;241m=\u001b[39mchi2, k\u001b[38;5;241m=\u001b[39mn)\u001b[38;5;66;03m#select best feature using chi squared\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m       fit1\u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mfit(indep_x,dep_y)\u001b[38;5;66;03m#create a model based on best feature\u001b[39;00m\n\u001b[0;32m      5\u001b[0m       selectk_features \u001b[38;5;241m=\u001b[39m fit1\u001b[38;5;241m.\u001b[39mtransform(indep_x)\u001b[38;5;66;03m#Create a new indep_x dataset based on k values\u001b[39;00m\n\u001b[0;32m      6\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m selectk_features\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:567\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    562\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    563\u001b[0m         X, y, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    564\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X, y)\n\u001b[1;32m--> 567\u001b[0m score_func_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_func(X, y)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpvalues_ \u001b[38;5;241m=\u001b[39m score_func_ret\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:270\u001b[0m, in \u001b[0;36mchi2\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m Y \u001b[38;5;241m=\u001b[39m LabelBinarizer(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    272\u001b[0m     Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:329\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(y)\u001b[38;5;241m.\u001b[39mtransform(y)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:306\u001b[0m, in \u001b[0;36mLabelBinarizer.fit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my has 0 samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_input_ \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39missparse(y)\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m unique_labels(y)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cluster\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:104\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    102\u001b[0m _unique_labels \u001b[38;5;241m=\u001b[39m _FN_UNIQUE_LABELS\u001b[38;5;241m.\u001b[39mget(label_type, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _unique_labels:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mrepr\u001b[39m(ys))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# array_api does not allow for mixed dtypes\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     unique_ys \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mconcat([_unique_labels(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys])\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([10. , 10. ,  9.2, ...,  6.2, 10. ,  6.3]),)"
     ]
    }
   ],
   "source": [
    "kbest=selectkbest(indep_x,dep_y,5) # call the fun & put k=5    \n",
    "\n",
    "acclin=[]#Create a empty list\n",
    "accsvml=[] \n",
    "accsvmnl=[]\n",
    "accdes=[]\n",
    "accrf=[]\n",
    "\n",
    "X_train, X_test, y_train, y_test=split_scalar(kbest,dep_y) #Kbest select best 5 feature,and split into training set and test set    \n",
    "for i in kbest:   \n",
    "    r2_lin=Linear(X_train,y_train,X_test)\n",
    "    acclin.append(r2_lin)\n",
    "    \n",
    "    r2_sl=svm_linear(X_train,y_train,X_test)    \n",
    "    accsvml.append(r2_sl)\n",
    "    \n",
    "    r2_NL=svm_NL(X_train,y_train,X_test)\n",
    "    accsvmnl.append(r2_NL)\n",
    "    \n",
    "    r2_d=Decision(X_train,y_train,X_test)\n",
    "    accdes.append(r2_d)\n",
    "    \n",
    "    r2_r=random(X_train,y_train,X_test)\n",
    "    accrf.append(r2_r)\n",
    "    \n",
    "    \n",
    "result=selectk_regression(acclin,accsvml,accsvmnl,accdes,accrf)#call selectk_regression fun then only the output comes in table format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908bd01-1986-4184-b4f2-1eb5c28d5a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
